{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project by Jeremy Bouhi & Lucas Trevalinet\n",
    "\n",
    "# Classification Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pyprind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 : Grab Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We consider you had already unzip the dataset file and that this file is in the parent folder\n",
    "\n",
    "#GLOBAL_PATH = os.path.join(\"Freemium_cass_global_20180315-170000\",\"20180315-170000\", \"juri\",\"cass\",\"global\") #Lucas' Path\n",
    "GLOBAL_PATH = os.path.join(\"..\",\"20180315-170000\", \"juri\",\"cass\",\"global\") #Jeremy's Path\n",
    "\n",
    "# We need to do that for excluding data from Juri_path\n",
    "CIVILE_PATH = os.path.join(GLOBAL_PATH,\"civile\")\n",
    "COMMERCIALE_PATH = os.path.join(GLOBAL_PATH,\"commerciale\")\n",
    "CRIMINELLE_PATH = os.path.join(GLOBAL_PATH,\"criminelle\")\n",
    "SOCIALE_PATH = os.path.join(GLOBAL_PATH,\"sociale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "ident = []\n",
    "text = []\n",
    "division = []\n",
    "#DATA_PATH = [SOCIALE_PATH] # For faster test\n",
    "DATA_PATH = [CIVILE_PATH, COMMERCIALE_PATH, CRIMINELLE_PATH, SOCIALE_PATH]\n",
    "\n",
    "for DIVISION_PATH in DATA_PATH :\n",
    "    xml_files = list(Path(DIVISION_PATH).glob('**/*.xml'))\n",
    "    \n",
    "    for xml_file in xml_files:\n",
    "    \n",
    "        with open(xml_file, 'r', encoding=\"utf-8\") as content:\n",
    "\n",
    "            etree = ET.parse(content) #create an ElementTree object \n",
    "            root = etree.getroot()\n",
    "            \n",
    "            # For getting the ID\n",
    "            for child in root.iter('META_COMMUN'):\n",
    "                id = child.find('ID').text\n",
    "                ident.append(id)\n",
    "\n",
    "            for child in root.iter('BLOC_TEXTUEL'):\n",
    "                contenu = \"\".join(child.itertext())\n",
    "                text.append(contenu)\n",
    "                \n",
    "            # For getting the division    \n",
    "            for child in root.iter('META_JURI_JUDI'):\n",
    "                formation = re.sub('CHAMBRE|_|[0-9]', '', child.find('FORMATION').text)\n",
    "                division.append(formation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 : Create DataFrame to manipulate easily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id': ident, 'text': text, 'division': division}\n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "# save for the next time\n",
    "df.to_pickle('3FD8KA7.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('3FD8KA7.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all empty texts\n",
    "df = df[df['text']!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You chose to run using fast_execution\n",
      "data size:  500\n"
     ]
    }
   ],
   "source": [
    "fast_execution = True #to run code faster\n",
    "\n",
    "if(fast_execution) : \n",
    "    df = df.sample(500)\n",
    "    print('You chose to run using fast_execution')\n",
    "    print('data size: ', df.shape[0])\n",
    "\n",
    "else :\n",
    "    print('You chose to run with the whole dataset')\n",
    "    print('data size: ', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 : Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing step pull off all the htlm tag, \"\\n\", and any special characters\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # remove HTML tags\n",
    "    text = re.sub('[,\\/#!$%\\^&\\*:{}=\\_`~()«»°–]','', text) # remove special characters except ; and . for spliting in sentences\n",
    "    text = text.replace('\\n','').replace('\\t','').replace('\\'',' ')\n",
    "    text = re.sub(r'\\s{2,}', ' ', text) # remove extra space\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We clean data on X, we don't modify directly the DataFrame to display the right sentences (after calculated whiches ones are the most influent by using X)\n",
    "\n",
    "X = df['text']\n",
    "y = df['division']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '.'' from lews articles (for example article L. 1256) are bothering for seperate our text in sentences\n",
    "def clean_references_to_law_articles(text) :\n",
    "    text = re.sub(r'((?:\\b[A-Z][\\.-]? ?)?(?:\\d+-?\\d+\\b))', 'N', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def clean_before_tokenize(text) :\n",
    "    text = clean_references_to_law_articles(text)\n",
    "    text = re.sub('\\.\\.\\.','', text) # remove ...\n",
    "    text = re.sub(r'\\bM\\. \\b','M ', text) # explicit replacement of M. (because of the .)\n",
    "    text = text.replace(' ;','. ') # some sentences are split by ;\n",
    "    return text\n",
    "\n",
    "def sent_tokenizer(text) : \n",
    "    text = clean_before_tokenize(text)\n",
    "    text = sent_tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(clean_before_tokenize)\n",
    "df['text'] = df['text'].apply(sent_tokenizer)\n",
    "\n",
    "# X and df['text'] are exactly the same here unless one thing : X[i] is a signle string, df['text'][i] is an array and each element represents an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_references_to_law_codes(text) : \n",
    "    return re.sub('(?<=code )civil|de l action sociale et des familles|de l artisanat|des assurances|de l aviation civile|du cinéma et de l image animée|de commerce|des communes( de la Nouvelle-Calédonie)?|de la consommation', '', text)\n",
    "\n",
    "X = X.apply(clean_references_to_law_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(clean_references_to_law_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all numbers\n",
    "\n",
    "def remove_numbers(text) :\n",
    "    text = re.sub(r'[0-9]*', '', text)\n",
    "    return text\n",
    "\n",
    "X = X.apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to lower case\n",
    "\n",
    "def to_lower_case(s) :\n",
    "    return s.lower()\n",
    "\n",
    "X = X.apply(to_lower_case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decided to replace any corresponding labels to a unique special one : *_* \n",
    "\n",
    "def clean_words_corresponding_to_labels(text) :\n",
    "    return re.sub(r'\\b(criminel(le)?|commercial(e)?|social(e)?|civil(e))\\b', '*_*', text)\n",
    "\n",
    "X = X.apply(clean_words_corresponding_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all words we don't care :\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \n",
    "    french_stopwords = set(stopwords.words('french'))\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    \n",
    "    content_tokens = \"\"\n",
    "    for token in tokens:\n",
    "        if token not in french_stopwords:\n",
    "            content_tokens += token\n",
    "            content_tokens += \" \"\n",
    "    return(content_tokens) \n",
    "\n",
    "X = X.apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most influential words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_civile = X.where(y=='CIVILE')\n",
    "X_commerciale = X.where(y=='COMMERCIALE')\n",
    "X_criminelle = X.where(y=='CRIMINELLE')\n",
    "X_sociale = X.where(y=='SOCIALE')\n",
    "\n",
    "#to remove NaN values\n",
    "X_civile = X_civile[~X_civile.isnull()]\n",
    "X_commerciale = X_commerciale[~X_commerciale.isnull()]\n",
    "X_criminelle = X_criminelle[~X_criminelle.isnull()]\n",
    "X_sociale = X_sociale[~X_sociale.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n",
      "78\n",
      "69\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# so they all have differents shapes\n",
    "print(X_civile.shape[0])\n",
    "print(X_commerciale.shape[0])\n",
    "print(X_criminelle.shape[0])\n",
    "print(X_sociale.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "def get_tfidf_matrix(X) : \n",
    "    \n",
    "    count_vect = CountVectorizer()\n",
    "    freq_term_matrix = count_vect.fit_transform(X)\n",
    "\n",
    "    tfidf = TfidfTransformer(norm=\"l2\")\n",
    "    doc_tfidf_matrix = tfidf.fit_transform(freq_term_matrix)\n",
    "    \n",
    "    return (doc_tfidf_matrix, count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_influential_words(X, top=10) :\n",
    "    \n",
    "    tfidf_matrix, vec = get_tfidf_matrix(X)\n",
    "    \n",
    "    tfidf_sorting = np.argsort(tfidf_matrix.toarray()).flatten()[::-1]\n",
    "    feature_array = np.array(vec.get_feature_names())\n",
    "    top_n = feature_array[tfidf_sorting][:top]\n",
    "    \n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_civile = get_influential_words(X_civile)\n",
    "top_commerciale = get_influential_words(X_commerciale)\n",
    "top_criminelle = get_influential_words(X_criminelle)\n",
    "top_sociale = get_influential_words(X_sociale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>civile</th>\n",
       "      <th>commerciale</th>\n",
       "      <th>criminelle</th>\n",
       "      <th>sociale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pension</td>\n",
       "      <td>acte</td>\n",
       "      <td>fils</td>\n",
       "      <td>frimigacci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alimentaire</td>\n",
       "      <td>omissions</td>\n",
       "      <td>père</td>\n",
       "      <td>blamable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>inexactitudes</td>\n",
       "      <td>mère</td>\n",
       "      <td>legerete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>visite</td>\n",
       "      <td>promesse</td>\n",
       "      <td>mineur</td>\n",
       "      <td>loi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>femme</td>\n",
       "      <td>epoux</td>\n",
       "      <td>parentale</td>\n",
       "      <td>lachize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ex</td>\n",
       "      <td>acquerir</td>\n",
       "      <td>enfant</td>\n",
       "      <td>nuire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>les</td>\n",
       "      <td>les</td>\n",
       "      <td>civilement</td>\n",
       "      <td>manifeste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>epoux</td>\n",
       "      <td>exploit</td>\n",
       "      <td>assises</td>\n",
       "      <td>preuve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conjugal</td>\n",
       "      <td>non</td>\n",
       "      <td>responsable</td>\n",
       "      <td>intention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>enfant</td>\n",
       "      <td>ete</td>\n",
       "      <td>autorité</td>\n",
       "      <td>etabli</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        civile    commerciale   criminelle     sociale\n",
       "0      pension           acte         fils  frimigacci\n",
       "1  alimentaire      omissions         père    blamable\n",
       "2           no  inexactitudes         mère    legerete\n",
       "3       visite       promesse       mineur         loi\n",
       "4        femme          epoux    parentale     lachize\n",
       "5           ex       acquerir       enfant       nuire\n",
       "6          les            les   civilement   manifeste\n",
       "7        epoux        exploit      assises      preuve\n",
       "8     conjugal            non  responsable   intention\n",
       "9       enfant            ete     autorité      etabli"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_division = {'civile': top_civile, 'commerciale': top_commerciale, 'criminelle': top_criminelle, 'sociale': top_sociale}\n",
    "words_df = pd.DataFrame(data = per_division)\n",
    "\n",
    "words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build document summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you'll build the tf-idf matrix for the text corpus, you will use the tf-idf of each word to compute a value for each sentence. The n top sentences for a document will be used to represent the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n"
     ]
    }
   ],
   "source": [
    "#if fast_execution was chosen, find the 1st right index of X (because we only took a sample)\n",
    "\n",
    "index = 0\n",
    "while (index not in X) :\n",
    "    index += 1\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized = X.apply(sent_tokenize)\n",
    "tfidf_matrix, vec = get_tfidf_matrix(X_tokenized[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sentences_for_document(formatted_X, real_X, index, top=5) :\n",
    "\n",
    "    top_sentences = []\n",
    "    sentence_score = []\n",
    "    tfidf_matrix, vec = get_tfidf_matrix(formatted_X[index])\n",
    "\n",
    "    for i in range(tfidf_matrix.shape[0]) :\n",
    "        mat = tfidf_matrix[i].toarray()\n",
    "\n",
    "        # The score of the sentence corresponds to the sum of the tf-idf of each word / number of words\n",
    "        sentence_score.append(np.sum(mat) / len(mat[mat > 0]))\n",
    "\n",
    "    print('score of each sentence : ', sentence_score) \n",
    "    top_sentence_score = np.argsort(sentence_score)[::-1][:top]\n",
    "    print('index of the top sentences which can summarize the document: ' ,top_sentence_score)\n",
    "\n",
    "    for i in range(top) : \n",
    "        top_sentences.append(real_X[index][top_sentence_score[i]])\n",
    "    \n",
    "\n",
    "    return top_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score of each sentence :  [0.15623339920194115, 0.1567667271658211, 0.3725512846138429, 0.1996243184851927, 0.18134733021715574, 0.1349693587617249, 0.17665500618206795, 0.17365259381537232, 0.13303555231114095, 0.4963229708433413, 0.3754581885089165, 0.2137402467354794, 0.23039986428680045, 0.06569902189108899, 1.0, 0.7071067811865476, 0.08481913551305212, 0.21157385472203713, 0.3725512846138429, 0.037003182971877616, 0.04675415271709843, 0.15862067097787377, 0.17665500618206795, 0.17341708628973831]\n",
      "index of the top sentences which can summarize the document:  [14 15  9 10  2]\n"
     ]
    }
   ],
   "source": [
    "top_sentences = get_top_sentences_for_document(X_tokenized, df['text'], index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top sentences of the document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remploi N.N €.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dépréciation du surplus N.N €.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D où il suit que le moyen n est pas fondé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PAR CES MOTIFS REJETTE le pourvoi Condamne Mme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>que le juge doit au besoin d office s assurer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       top sentences of the document\n",
       "0                                     Remploi N.N €.\n",
       "1                     Dépréciation du surplus N.N €.\n",
       "2         D où il suit que le moyen n est pas fondé.\n",
       "3  PAR CES MOTIFS REJETTE le pourvoi Condamne Mme...\n",
       "4  que le juge doit au besoin d office s assurer ..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data = {'top sentences of the document' : top_sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences of this document:  24\n",
      "top sentences:  ['Remploi N.N €.', 'Dépréciation du surplus N.N €.', 'D où il suit que le moyen n est pas fondé.', 'PAR CES MOTIFS REJETTE le pourvoi Condamne Mmes H et Nicole X aux dépens.', 'que le juge doit au besoin d office s assurer que ces dispositions ont été respectées.']\n"
     ]
    }
   ],
   "source": [
    "# To display entirely\n",
    "print('number of sentences of this document: ', len(X_tokenized[index]))\n",
    "print('top sentences: ', top_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y shape <class 'pandas.core.series.Series'> (500,)\n",
      "X shape <class 'pandas.core.series.Series'> (500,)\n",
      "84492    rejet pourvoi x alexandre contre arret chambre...\n",
      "57150    attendu arret confirmatif attaque accueillant ...\n",
      "44718    moyen unique pris divers griefs attendu honore...\n",
      "68077    moyen unique pris quatre branches attendu mme ...\n",
      "58659    recevabilite pourvoi attendu voie cassation ou...\n",
      "Name: text, dtype: object\n",
      "72984     moyen unique pris deux branches attendu fait g...\n",
      "100422    cour cassation chambre *_* a rendu arrêt suiva...\n",
      "94673     statuant pourvoi forme - procureur general pre...\n",
      "1318      cour cassation deuxième chambre *_* a rendu ar...\n",
      "113347    moyen unique vu les articles n- code travail ....\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Y shape\", type(y),y.shape)\n",
    "print(\"X shape\", type(X),X.shape)\n",
    "print(X.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size = 0.2 ,random_state=42 )\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CIVILE' 'COMMERCIALE' 'CRIMINELLE' 'SOCIALE']\n"
     ]
    }
   ],
   "source": [
    "### A RAJOUTER #####\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "type(X_train_counts)\n",
    "#print(X_train_counts)\n",
    "tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_transformed = tf_transformer.transform(X_train_counts)\n",
    "#print(type(X_train_transformed))\n",
    "#print(X_train_transformed)\n",
    "\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_transformed = tf_transformer.transform(X_test_counts)\n",
    "#print(type(X_test_transformed))\n",
    "#print(X_test_transformed)\n",
    "\n",
    "labels = LabelEncoder()\n",
    "y_train_labels_fit = labels.fit(y_train)\n",
    "y_train_labels_trf = labels.transform(y_train)\n",
    "\n",
    "\n",
    "print(labels.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 13155)\n",
      "pred [0 2 0 0 0 1 0 0 0 1 1 3 0 0 0 3 2 0 3 1 0 3 0 0 0 3 0 2 0 3 0 2 0 3 0 0 2\n",
      " 3 1 0 0 3 2 0 2 0 2 0 3 2 2 0 3 0 2 1 3 0 3 2 0 0 1 0 0 3 3 0 0 3 1 1 0 0\n",
      " 1 3 0 2 0 0 0 0 0 0 0 0 2 0 0 0 0 3 2 0 0 2 0 0 3 0]\n",
      " (100,)\n",
      "test 116253        SOCIALE\n",
      "95765      CRIMINELLE\n",
      "34941          CIVILE\n",
      "68607     COMMERCIALE\n",
      "38022          CIVILE\n",
      "74404     COMMERCIALE\n",
      "14571          CIVILE\n",
      "71617     COMMERCIALE\n",
      "31935          CIVILE\n",
      "62215     COMMERCIALE\n",
      "70576     COMMERCIALE\n",
      "125993        SOCIALE\n",
      "13480          CIVILE\n",
      "52490          CIVILE\n",
      "26071          CIVILE\n",
      "128313        SOCIALE\n",
      "90547      CRIMINELLE\n",
      "25663          CIVILE\n",
      "129043        SOCIALE\n",
      "72619     COMMERCIALE\n",
      "31827          CIVILE\n",
      "117049        SOCIALE\n",
      "28879          CIVILE\n",
      "28045          CIVILE\n",
      "45163          CIVILE\n",
      "107247        SOCIALE\n",
      "118193        SOCIALE\n",
      "84376      CRIMINELLE\n",
      "58338          CIVILE\n",
      "122670        SOCIALE\n",
      "             ...     \n",
      "66577     COMMERCIALE\n",
      "72789     COMMERCIALE\n",
      "4028           CIVILE\n",
      "9181           CIVILE\n",
      "64527     COMMERCIALE\n",
      "118391        SOCIALE\n",
      "40046          CIVILE\n",
      "91854      CRIMINELLE\n",
      "27625          CIVILE\n",
      "14469          CIVILE\n",
      "62883     COMMERCIALE\n",
      "4930           CIVILE\n",
      "109862        SOCIALE\n",
      "1230           CIVILE\n",
      "5542           CIVILE\n",
      "52809          CIVILE\n",
      "90037      CRIMINELLE\n",
      "54836          CIVILE\n",
      "44794          CIVILE\n",
      "8346           CIVILE\n",
      "7640           CIVILE\n",
      "114066        SOCIALE\n",
      "94891      CRIMINELLE\n",
      "11485          CIVILE\n",
      "40180          CIVILE\n",
      "90230      CRIMINELLE\n",
      "119256        SOCIALE\n",
      "36846          CIVILE\n",
      "36073          CIVILE\n",
      "133007        SOCIALE\n",
      "Name: division, Length: 100, dtype: object\n",
      "accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "#### A RAJOUTER ####\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "linear_svc = LinearSVC()\n",
    "clf = linear_svc.fit(X_train_transformed,y_train_labels_trf)\n",
    "\n",
    "calibrated_svc = CalibratedClassifierCV(base_estimator = linear_svc, cv = \"prefit\")\n",
    "calibrated_svc.fit(X_train_transformed,y_train_labels_trf)\n",
    "predicted = calibrated_svc.predict(X_test_transformed)\n",
    "print(X_test_transformed.shape)\n",
    "\n",
    "print(\"pred\",predicted)\n",
    "print(\"\",y_test.shape)\n",
    "print(\"test\",y_test[0:100])\n",
    "\n",
    "acc = accuracy_score(y_test, predicted)\n",
    "print(\"accuracy\",acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
